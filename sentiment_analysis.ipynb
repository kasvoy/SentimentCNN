{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, random\n",
    "\n",
    "\"\"\"\n",
    "The sentiment analysis model within this notebook uses publicly available datasets:\n",
    "\n",
    "1. Large Movie Review Dataset: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "2. Sentiment Polarity Dataset (v1 and v2): https://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n",
    "All data is contained in the \"data\" directory (not committed in this repo).\n",
    "The code below assumes the same directory structure as the original datasets under the root \"data\" folder.\n",
    "\"\"\"\n",
    "\n",
    "IMDB_DATA_PATH        = \"data/aclImdb/\"\n",
    "POLARITY_v1_DATA_PATH = \"data/rt-polaritydata/rt-polaritydata/\"\n",
    "POLARITY_v2_DATA_PATH = \"data/review_polarity/txt_sentoken\"\n",
    "\n",
    "def load_inidv_dataset(set_path: str) -> tuple:\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in ['pos', 'neg']:\n",
    "        cat_path = os.path.join(set_path, label)\n",
    "        for file_name in os.listdir(cat_path):\n",
    "            file_path = os.path.join(cat_path, file_name)\n",
    "            with open(file_path, 'r') as file:\n",
    "                text = file.read()\n",
    "                texts.append(text)\n",
    "            labels.append(0 if label=='neg' else 1)\n",
    "            \n",
    "    return (texts, labels)\n",
    "\n",
    "\n",
    "def load_dataset(train_path: str, test_path: str, seed=1) -> tuple:\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    \n",
    "    (train_texts, train_labels) = load_inidv_dataset(set_path=train_path)\n",
    "    (test_texts, test_labels)   = load_inidv_dataset(set_path=test_path)\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.shuffle(test_texts)\n",
    "    \n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "    random.shuffle(test_labels)\n",
    "    \n",
    "    #remove html tags from the texts\n",
    "    train_texts = [re.sub('<.*?>', '', text) for text in train_texts]\n",
    "    test_texts  = [re.sub('<.*?>', '', text) for text in test_texts]\n",
    "    \n",
    "    return ((train_texts, train_labels), (test_texts, test_labels))\n",
    "\n",
    "\n",
    "def load_polarity(v1_path, v2_path):\n",
    "    \n",
    "    v1_file_names = os.listdir(v1_path)\n",
    "    \n",
    "    v1_texts = []\n",
    "    v1_labels = []\n",
    "    \n",
    "    for file_name in v1_file_names:\n",
    "        file_path = os.path.join(v1_path, file_name)\n",
    "        label = (0 if 'neg' in file_name else 1)\n",
    "\n",
    "        with open(file_path, 'r', errors='ignore') as file:\n",
    "            text = file.read()\n",
    "\n",
    "            for snippet in text.splitlines():\n",
    "                v1_texts.append(snippet)\n",
    "                v1_labels.append(label)\n",
    "\n",
    "   \n",
    "    (v2_texts, v2_labels) = load_inidv_dataset(set_path=POLARITY_v2_DATA_PATH)\n",
    "    \n",
    "    return ((v1_texts, v1_labels), (v2_texts, v2_labels))\n",
    "\n",
    "def get_smaller_dataset(size: int, texts: list[str], labels: list[int], seed=10) -> tuple:\n",
    "    \n",
    "    random.seed(seed)\n",
    "    smaller_texts = random.sample(texts, size)\n",
    "    random.seed(seed)\n",
    "    smaller_labels = random.sample(labels, size)\n",
    "\n",
    "    return (smaller_texts, smaller_labels)\n",
    "\n",
    "\n",
    "train_path = os.path.join(IMDB_DATA_PATH, \"train\")\n",
    "test_path  = os.path.join(IMDB_DATA_PATH, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main training set - Large Movie Review Dataset (IMDB)\n",
    "((train_texts, train_labels), (test_texts, test_labels)) = load_dataset(train_path=train_path, test_path=test_path)\n",
    "\n",
    "#Review Polarity Datasets - used as additional test data\n",
    "(v1_texts, v1_labels), (v2_texts, v2_labels) = load_polarity(v1_path=POLARITY_v1_DATA_PATH, v2_path=POLARITY_v2_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "MAX_FEATURES = 5000\n",
    "\n",
    "random.seed()\n",
    "seed = random.randint(1, 1000)\n",
    "\n",
    "#(sm_texts, sm_labels) = get_smaller_dataset(12000, train_texts, train_labels, seed=seed)\n",
    "\n",
    "#encoding largest dataset, training vectorizer and selector --> to be used for encoding other datasets\n",
    "def get_ngram_dataset(train_texts, train_labels, test_texts, test_labels, min_df=5):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=min_df) \n",
    "    \n",
    "    X_train = vectorizer.fit_transform(train_texts)  \n",
    "    selector = SelectKBest(score_func=f_classif, k=min(MAX_FEATURES, X_train.shape[1]))    \n",
    "\n",
    "    y_train = np.array(train_labels)\n",
    "    y_test = np.array(test_labels)\n",
    "    \n",
    "    X_train = selector.fit_transform(X_train, y_train).toarray()\n",
    "\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "    X_test = selector.transform(X_test).toarray()\n",
    "    \n",
    "    return ((X_train, y_train), (X_test, y_test), (vectorizer, selector))\n",
    "\n",
    "#get the encoded data as well as the trained vectorizer and selector \n",
    "(X_train, y_train), (X_test, y_test), (vectorizer, selector) = get_ngram_dataset(\n",
    "    train_texts, train_labels, test_texts, test_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(texts, labels, vectorizer, selector):\n",
    "    X = vectorizer.transform(texts)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    X = selector.transform(X).toarray()\n",
    "    \n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v1_test, y_v1_test = vectorize(v1_texts, v1_labels, vectorizer=vectorizer, selector=selector)\n",
    "X_v2_test, y_v2_test = vectorize(v2_texts, v2_labels, vectorizer=vectorizer, selector=selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholder benchmark naive bayes model\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_sm_train, X_sm_test, y_sm_train, y_sm_test = train_test_split(X_sm, y_sm, test_size=0.33, random_state=42)\n",
    "\n",
    "clf = ComplementNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(accuracy_score(clf.predict(X_v1_test), y_v1_test))\n",
    "print(accuracy_score(clf.predict(X_v2_test), y_v2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#For now just wrapping already processed vectors\n",
    "class NgramMovieDataset(Dataset):\n",
    "    def __init__(self, X, y, device):        \n",
    "        self.x = torch.from_numpy(X).to(device, dtype=torch.float32)\n",
    "        self.y = torch.from_numpy(y).to(device, dtype=torch.float32)\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_dataloader(X, y, device, batch_size=64, shuffle=True):\n",
    "    dataset = NgramMovieDataset(X=X, y=y, device=device)\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# train_dataloader = get_dataloader(X=X_train, y=y_train, device=device)\n",
    "# test_dataloader  = get_dataloader(X=X_test, y=y_test, device=device)\n",
    "\n",
    "# v1_test_dataloader = get_dataloader(X=X_v1_test, y=y_v1_test, device=device)\n",
    "# v2_test_dataloader = get_dataloader(X=X_v2_test, y=y_v2_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "class SentimentCLF(nn.Module):\n",
    "    def __init__(self, n_units):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, n_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_units, n_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_units, 1)\n",
    "        )\n",
    "            \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        dropped = self.dropout(x)\n",
    "        logits = self.linear_stack(dropped)\n",
    "        \n",
    "        return self.sigmoid(logits)  \n",
    "    \n",
    "\n",
    "def train(dataloader: DataLoader, model: nn.Module, loss_fn, optimizer):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader): \n",
    "        model.train()\n",
    "        \n",
    "        pred = model(X).squeeze()    \n",
    "        loss = loss_fn(pred, y)\n",
    "         \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "                   \n",
    "                     \n",
    "def test(dataloader: DataLoader, model: nn.Module, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X).squeeze()\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            out_class = (pred > 0.5).float()      \n",
    "            correct += (out_class==y).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentimentCLF().to(device)\n",
    "# loss_fn   = nn.BCELoss()\n",
    "# optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "\n",
    "# epochs = 15\n",
    "# for t in range(epochs):\n",
    "#     print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "#     train(train_dataloader, model, loss_fn, optimizer)\n",
    "#     #test(test_dataloader, model, loss_fn)\n",
    "# print(\"Done!\")\n",
    "\n",
    "# for dataloader in [test_dataloader, v1_test_dataloader, v2_test_dataloader]:\n",
    "#     test(dataloader=dataloader, model=model, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    module=SentimentCLF,\n",
    "    module__n_units=512,\n",
    "    lr = 0.001,\n",
    "    criterion=nn.BCELoss,   \n",
    "    device='cuda',\n",
    "    max_epochs=15,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sk = torch.from_numpy(X_train).to(dtype=torch.float32)\n",
    "y_train_sk = torch.from_numpy(y_train).to(dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize()\n",
    "net.module_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(X_train_sk, y_train_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sk = torch.from_numpy(X_test).to(device=device, dtype=torch.float32)\n",
    "y_test_sk = torch.from_numpy(y_test).to(device='cpu', dtype=torch.float32)\n",
    "\n",
    "pred = net.predict(X_test_sk)\n",
    "accuracy_score(pred, y_test_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_1_sk = torch.from_numpy(X_v1_test).to(device=device, dtype=torch.float32)\n",
    "accuracy_score(net.predict(X_test_1_sk), y_v1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# deactivate skorch-internal train-valid split and verbose logging\n",
    "net.set_params(train_split=False, verbose=0)\n",
    "param_grid = {\n",
    "    'lr': [0.001, 0.01],\n",
    "    'max_epochs': [10, 15, 20],\n",
    "    'module__n_units': [100, 500, 1000]\n",
    "}\n",
    "gs = GridSearchCV(net, param_grid=param_grid, refit=False, cv=3, scoring='accuracy', verbose=2)\n",
    "\n",
    "gs.fit(X_train_sk, y_train_sk)\n",
    "#print(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))\n",
    "\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
