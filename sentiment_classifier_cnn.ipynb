{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The sentiment analysis model within this notebook uses publicly available datasets:\n",
    "\n",
    "1. Large Movie Review Dataset: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "2. Sentiment Polarity Dataset (v1 and v2): https://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "3. Rotten Tomatoes web scraped critic reviews: https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataset_utils import *\n",
    "\n",
    "train_path = os.path.join(IMDB_DATA_PATH, \"train\")\n",
    "test_path  = os.path.join(IMDB_DATA_PATH, \"test\")\n",
    "\n",
    "\"\"\"\n",
    "For details on implementation of dataset loading and other utils check out dataset_utils.py\n",
    "\n",
    "CONVENTION: label 0 --> negative review\n",
    "            label 1 --> positive review\n",
    "\"\"\"\n",
    "\n",
    "#Main training set - Large Movie Review Dataset (IMDB)\n",
    "((imdb_train_texts, imdb_train_labels), (imdb_test_texts, imdb_test_labels)) = load_imdb_dataset(train_path=train_path, test_path=test_path)\n",
    "\n",
    "#Review Polarity Datasets\n",
    "(v1_texts, v1_labels), (v2_texts, v2_labels) = load_polarity(v1_path=POLARITY_v1_DATA_PATH, v2_path=POLARITY_v2_DATA_PATH)\n",
    "\n",
    "#Rotten tomatoes critic dataset\n",
    "rotten_train_texts, rotten_train_labels, short_rotten_train, short_rotten_test, random_rotten_test, pos_rotten_test, neg_rotten_test = load_rotten_split(ROTTEN_PATH, n_train_samples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "short_rotten_train_texts = [text for _, text in short_rotten_train]\n",
    "short_rotten_train_labels = [label for label, _ in short_rotten_train]\n",
    "\n",
    "train_texts_v1, test_texts_v1, train_labels_v1, test_labels_v1 = train_test_split(v1_texts, v1_labels, test_size=0.33, random_state=42)\n",
    "train_texts_v2, test_texts_v2, train_labels_v2, test_labels_v2 = train_test_split(v2_texts, v2_labels, test_size=0.33, random_state=42)\n",
    " \n",
    "train_texts = imdb_train_texts + rotten_train_texts + train_texts_v1 + train_texts_v2\n",
    "train_labels = imdb_train_labels + rotten_train_labels + train_labels_v1 + train_labels_v2\n",
    "\n",
    "\n",
    "seed=10\n",
    "random.seed(seed)\n",
    "random.shuffle(train_texts)\n",
    "random.seed(seed)\n",
    "random.shuffle(train_labels)\n",
    "\n",
    "print(f\"Training set size: {len(train_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building vocabulary and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "MAX_FEATURES = 100000\n",
    "TARGET_LENGTH = 250\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), lowercase=True, max_features=MAX_FEATURES)\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "vocab = vectorizer.vocabulary_\n",
    "remapped_vocab = {ngram: idx+1 for idx, (ngram, _) in enumerate(sorted(vocab.items(),key=lambda x: x[1]))}\n",
    "\n",
    "#adding tokens for padding and Out Of Vocabulary to the vocab\n",
    "remapped_vocab['<pad>'] = 0\n",
    "remapped_vocab['<OOV>'] = len(remapped_vocab)\n",
    "\n",
    "inverse_vocab = {idx: ngram for ngram, idx in remapped_vocab.items()}\n",
    "\n",
    "tokenizer = vectorizer.build_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "vocab_size = len(remapped_vocab)\n",
    "\n",
    "class SentimentCNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, out_channels, kernel_size1, kernel_size2, kernel_size3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=out_channels, kernel_size=kernel_size1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=embedding_dim, out_channels=out_channels, kernel_size=kernel_size2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=embedding_dim, out_channels=out_channels, kernel_size=kernel_size3)\n",
    "        \n",
    "        self.conv_stack1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=out_channels, kernel_size=kernel_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout1d(p=0.2)\n",
    "        ) \n",
    "        \n",
    "        self.conv_stack2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=out_channels, kernel_size=kernel_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout1d(p=0.2)\n",
    "        )\n",
    "        \n",
    "        self.conv_stack3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=out_channels, kernel_size=kernel_size3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout1d(p=0.2)\n",
    "        )\n",
    "    \n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=600, out_features=300),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(in_features=300, out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(in_features=100, out_features=1),\n",
    "        )\n",
    "\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embed = self.embedding(x)\n",
    "        embed = embed.transpose(1,2)\n",
    "        \n",
    "        conv1 = self.conv_stack1(embed)\n",
    "        conv2 = self.conv_stack2(embed)\n",
    "        conv3 = self.conv_stack3(embed)\n",
    "        \n",
    "        pool1 = F.max_pool1d(conv1, kernel_size=conv1.shape[2]).squeeze(2)\n",
    "        pool2 = F.max_pool1d(conv2, kernel_size=conv2.shape[2]).squeeze(2)\n",
    "        pool3 = F.max_pool1d(conv3, kernel_size=conv3.shape[2]).squeeze(2)\n",
    "        \n",
    "        cat = torch.cat((pool1, pool2, pool3), dim=1)\n",
    "        \n",
    "        return self.linear_stack(cat)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, analyzer, vocab):\n",
    "    analyzed_list = analyzer(text)\n",
    "\n",
    "    unigrams = [t for t in analyzed_list if len(t.split()) == 1]\n",
    "    bigrams = [t for t in analyzed_list if len(t.split()) == 2]\n",
    "    \n",
    "    bigram_indices = [vocab.get(bigram) for bigram in bigrams]\n",
    "    unigram_indices = [vocab.get(unigram) for unigram in unigrams]\n",
    "    \n",
    "    filled = [0 if idx is None else idx for idx in unigram_indices]\n",
    "    \n",
    "    return torch.tensor(filled)\n",
    "\n",
    "\n",
    "def vectorize_mono(text, tokenizer, vocab):\n",
    "    tokenized = tokenizer(text)\n",
    "    index_vector = [vocab.get(mono) for mono in tokenized]\n",
    "    \n",
    "    filled = [vocab.get('<OOV>') if idx is None else idx for idx in index_vector]\n",
    "    return torch.tensor(filled)\n",
    "\n",
    "\n",
    "def tensor_to_text(tensor, inverse_vocab):\n",
    "    return [inverse_vocab.get(val.item()) for val in tensor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_train = [vectorize_mono(text, tokenizer=tokenizer, vocab=remapped_vocab) for text in train_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(tensor_list, target_length, padding_value=0):\n",
    "    padded_tensors = []\n",
    "    \n",
    "    for tensor in tensor_list:\n",
    "        pad_length = target_length - len(tensor)\n",
    "        padded_tensor = F.pad(tensor, (0, pad_length), value=padding_value)\n",
    "        \n",
    "        padded_tensors.append(padded_tensor.to(dtype=torch.int64))\n",
    "        \n",
    "    return torch.stack(padded_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train = pad_sequences(vectorized_train, target_length=TARGET_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import Checkpoint, LoadInitState, LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, LinearLR, StepLR, ExponentialLR\n",
    "\n",
    "model_dirs = sorted(os.listdir('models'))\n",
    "\n",
    "#top experiment group directory\n",
    "dir_idx = 3\n",
    "topdir = model_dirs[dir_idx]\n",
    "\n",
    "#experiments within chosen top experiment group directory\n",
    "experiments = sorted(os.listdir(f'models/{topdir}'))\n",
    "\n",
    "if len(experiments) == 0:\n",
    "    curr_number = 1\n",
    "else:\n",
    "    curr_number = int(experiments[-1][-1])\n",
    "    \n",
    "next_number = curr_number + 1\n",
    "\n",
    "dirname_curr=f'models/{topdir}/exp{curr_number}'\n",
    "f_pickle_curr=f'exp{curr_number}.pkl'\n",
    "\n",
    "dirname_next=f'models/{topdir}/exp{next_number}'\n",
    "f_pickle_next=f'exp{next_number}.pkl'\n",
    "\n",
    "#checkpoints for saving the model during training that track the validation loss. used for early stopping.\n",
    "#cp_current corresponds to the last experiment.\n",
    "#When we switch the callback to cp_next, this starts a new experiment\n",
    "cp_current = Checkpoint(monitor='valid_loss_best', dirname=dirname_curr, f_pickle=f_pickle_curr)\n",
    "cp_next    = Checkpoint(monitor='valid_loss_best', dirname=dirname_next, f_pickle=f_pickle_next)\n",
    "load_state = LoadInitState(cp_current)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    module=SentimentCNN,\n",
    "    module__kernel_size1=2,\n",
    "    module__kernel_size2=4,\n",
    "    module__kernel_size3=6,\n",
    "    module__embedding_dim=150,\n",
    "    module__out_channels=200,\n",
    "    lr = 0.001,\n",
    "    criterion=nn.BCEWithLogitsLoss,   \n",
    "    device=device,\n",
    "    max_epochs=15,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer__weight_decay=0.0001,\n",
    "    batch_size=32,\n",
    "    iterator_train__shuffle=False,\n",
    "    callbacks = [cp_current, load_state],\n",
    "    #callbacks = [cp_next],\n",
    ")\n",
    "\n",
    "net.fit(X=padded_train, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.load_params(checkpoint=cp_current)\n",
    "net.load_params(checkpoint=cp_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_model_byexp_n(exp_number, exp_dir_idx):\n",
    "\n",
    "    \"\"\"\n",
    "    A function loading the model from previously ran experiments.\n",
    "    \n",
    "    Parameters:\n",
    "    True\n",
    "    \n",
    "    model - a skorch model.\n",
    "    exp_path - path to the chosen experiment directory. Used during testing for saving results.\n",
    "    \"\"\"\n",
    "    \n",
    "    topdir = model_dirs[exp_dir_idx]\n",
    "\n",
    "    experiments = sorted(os.listdir(f'models/{topdir}'))\n",
    "    if len(experiments) == 0:\n",
    "        print(\"no experiments here\")\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            exp = experiments[exp_number-1]\n",
    "        except IndexError:\n",
    "            print(\"No such experiment\")\n",
    "            return None\n",
    "        \n",
    "        #path to experiment directory\n",
    "        exp_path = f'models/{topdir}/{exp}'\n",
    "        #path to model within exp_path        \n",
    "        pkl_path = os.path.join(exp_path, f'{exp}.pkl')\n",
    "\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            \n",
    "        return model, exp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, exp_path = get_model_byexp_n(exp_number=curr_number, exp_dir_idx=dir_idx)\n",
    "exp_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_test_v1 = [(test_labels_v1[index], text) for index, text in enumerate(test_texts_v1) if len(text.split(\" \")) < 25]\n",
    "rest_test_v1 = [(test_labels_v1[index], text) for index, text in enumerate(test_texts_v1) if len(text.split(\" \")) >= 25]\n",
    "\n",
    "long_test_v2 = [(test_labels_v2[index], text) for index, text in enumerate(test_texts_v2) if len(text.split(\" \")) > 300]\n",
    "long_test_imdb = [(imdb_test_labels[index], text) for index, text in enumerate(imdb_test_texts) if len(text.split(\" \")) > 300]\n",
    "\n",
    "long_test = long_test_v2 + long_test_imdb\n",
    "\n",
    "random.seed(seed)\n",
    "random.shuffle(long_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the imdb test reviews and their label into 3 datasets based on size:\n",
    "#small-1000, medium-5000, large-19000\n",
    "imdb_small_test_texts  = imdb_test_texts[:1000]\n",
    "imdb_small_test_labels = imdb_test_labels[:1000]\n",
    "\n",
    "imdb_medium_test_texts  = imdb_test_texts[1000:6000]\n",
    "imdb_medium_test_labels = imdb_test_labels[1000:6000]\n",
    "\n",
    "imdb_large_test_texts  = imdb_test_texts[6000:]\n",
    "imdb_large_test_labels = imdb_test_labels[6000:]\n",
    "\n",
    "short_test_v1_texts  = [text for _, text in short_test_v1]\n",
    "short_test_v1_labels = [label for label, _ in short_test_v1]\n",
    "\n",
    "short_test_rotten_texts = [text for _, text in short_rotten_test]\n",
    "short_test_rotten_labels = [label for label, _ in short_rotten_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding test reviews - vectorizing and padding\n",
    "\n",
    "encoded_test_sets = []\n",
    "for (texts, labels, name) in [(imdb_small_test_texts, imdb_small_test_labels, 'imdb_sm'), (imdb_medium_test_texts, imdb_medium_test_labels, 'imdb_md'),\n",
    "                            (imdb_large_test_texts, imdb_large_test_labels, 'imdb_lg'), (short_test_v1_texts, short_test_v1_labels, 'short_v1'),\n",
    "                            (short_test_rotten_texts, short_test_rotten_labels, 'short_rotten'),\n",
    "                            ([text for _, text in random_rotten_test], [label for label, _ in random_rotten_test], 'rotten_mix'),\n",
    "                            ([text for _, text in long_test], [label for label, _ in long_test], 'long (v2+imdb)'),\n",
    "                            ([text for _, text in rest_test_v1], [label for label, _ in rest_test_v1], 'rest v1(more than 25 tokens)'),\n",
    "                            (v2_texts, v2_labels, 'entire v2 test'),\n",
    "                            ([text for _, text in pos_rotten_test], [label for label, _ in pos_rotten_test], 'rotten_pos_only'),\n",
    "                            ([text for _, text in neg_rotten_test], [label for label, _ in neg_rotten_test], 'rotten_neg_only')]:\n",
    "\n",
    "    vectorized = [vectorize_mono(text, tokenizer=tokenizer, vocab=remapped_vocab) for text in texts]\n",
    "\n",
    "    #padding all text vectors to equal length equal to the target length during training.    \n",
    "    X_test = pad_sequences(vectorized, target_length=TARGET_LENGTH)\n",
    "    \n",
    "    y_test=torch.tensor(labels, dtype=torch.float32).unsqueeze(1)\n",
    "    encoded_test_sets.append((X_test, y_test, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "The main single-number metric we're checking is accuracy. We show the confusion matrix on each test set as well as the precision score.\n",
    "Precision metric answers the question \"What proportion of positive predictions were classified correctly?\".\n",
    "\"\"\"\n",
    "\n",
    "def test_loop(X, y, set_name, model):\n",
    "    print(f\"DATASET: {set_name}\")\n",
    "    \n",
    "    pred = model.predict(X)\n",
    "    \n",
    "    acc = accuracy_score(y, pred)\n",
    "    precision = precision_score(y, pred)\n",
    "    cm = confusion_matrix(y, pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    \n",
    "    print(f\"Accuracy score: {acc}\")\n",
    "    print(f\"Precision score: {precision}\")\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    \n",
    "    print('-----------------------------------------------------')\n",
    "    \n",
    "    return (acc, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "\n",
    "for (X_test, y_test, name) in encoded_test_sets:\n",
    "    acc, precision = test_loop(X_test, y_test, set_name=name, model=model)\n",
    "    acc_key = f\"{name}_acc\"\n",
    "    prec_key = f\"{name}_prec\"\n",
    "    \n",
    "    results_dict[acc_key] = acc\n",
    "    results_dict[prec_key] = precision   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#utility function to save the results on the test dataset in the experiment directory\n",
    "def write_results(exp_path, results_dict):\n",
    "    \n",
    "    res_path = os.path.join(exp_path, 'results.json')\n",
    "    json_data = json.dumps(results_dict, indent=1)\n",
    "    \n",
    "    if os.path.exists(res_path):\n",
    "        print(\"Results file already exists for this experiment. Aborting\")\n",
    "        return None\n",
    "    \n",
    "    with open(res_path, 'w') as f:\n",
    "        f.write(json_data)\n",
    "        \n",
    "write_results(exp_path=exp_path, results_dict=results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from training\n",
    "\n",
    "\n",
    "- Much better learning going from embedding_dim=50 to embedding_dim=100. seems like the complexity of the word vectors is a plus.\n",
    "- l2 regularization (optimizer weight decay) very helpful for avoiding overfitting\n",
    "- slow to train on conv kernel size=4 compared to 8. maybe because most training data is long or capturing the context is more important?\n",
    "- in pooling entire length, training unblocked after using 2 CNN layers instead of 3 (4, 8 kernel sizes for CNN)\n",
    "- in pooling entire length, training blocked after changing out_channels to 150 from 100 (6, 10 kernel sizes for CNN)\n",
    "- in general, if a model doesnt start learning, turning off l2 regu works sometimes.\n",
    "- completely opposed to above, sometimes cranking up l2 regu makes model start learning\n",
    "- as can be seen below, increasing target length was the best thing for performance\n",
    "- On the rotten dataset: when I don't give the model as many training samples from the rotten dataset, it identifies true reviews as true (high precision), but on negative reviews it almost guessses.\n",
    "\n",
    "\n",
    "## 1. Pooling entire length - notes:\n",
    "**Pooling one max value out of each feature map (pool kernel_size = length of seq after covolution). Catting the results and putting it through linear stack**\n",
    "\n",
    "\n",
    "### Target length 100, vocab size 50k (*wholepool_t100_emb50k*)\n",
    "\n",
    "1. exp1: k1=4, k2=8, target_len=100, embedding_dim=100, out_channels=100, relu only on conv (no stack). 3 linear layers (started with 2, see conclusion 1) (with dropout p=0.4 between).\n",
    "    - didn't learn (losses not improving), even after changing lr and l2 regu.\n",
    "    \n",
    "    **Exp 1 conclusion 1** - started learning **after changing linear stack to have 3 layers instead of 2**: 200-100-50-1, rather than 200-100-1.\n",
    "\n",
    "    Results after 24 epochs : best validation loss 0.4619, best valid acc 0.78.\n",
    "    Test results out of best valid loss CP in json.\n",
    "\n",
    "\n",
    "2. exp2: convstacks (conv-relu-dropout1d(0.2), started with regular dropout@0.5) instead of relu only. rest the same.\n",
    "\n",
    "    **Exp2 conclusion** The network didn't learn, even when dropout p was reduced, or dropout omitted altogether.\n",
    "    So even with a setup conceptually equivalent to exp1, where the network did learn, this one didn't. Still dont fully understand the reason.\n",
    "\n",
    "    **Changing dropout to dropout1d** made the network starting to learn (with convstacks dropout1d p =0.2).\n",
    "    After this change, the networks best valid loss and acc are basically the same as with exp1.\n",
    "\n",
    "    As results.json for exp2 shows, **the results are consistently better on all sets compared to exp1.**\n",
    "\n",
    "\n",
    "3. exp3: same as exp2 (stacks with d1d p=0.2) **+ batch norm AFTER relu** (conv-relu-dropout-batchnorm).\n",
    "    **Exp 3 conclusion** - pretty much the same in terms of training performance. Main meaningful diff in testing is that it does better on neg only and worse on pos only.\n",
    "\n",
    "    When changed order in stack to conv-batchnorm-relu-dropout network refused to start learning (even after changing lr and l2)\n",
    "    Maybe the diff will kick into gear with some higher level change, like vocab size or seq length, but i doubt it.\n",
    "\n",
    "\n",
    "4. exp4: k1=2, k2=4, rest same as exp2. Similar results as exp2, did terribly on neg only, and really well on pos only. When added batch norm, completely refused to learn\n",
    "\n",
    "5. exp5: same as exp4 - k1=2, k2=4, out_channels changed from 100 to 150, first linear layer has 300 units accordingly.\n",
    "\n",
    "    **Exp 5 conclusions** - results slightly better. Again, better on neg only and worse on pos only.\n",
    "    Seems like the smaller kernel sizes have results at least as good as the larger. So far for all of the experiments the best valid loss is 0.46 and valid acc is 0.78. No experiment got better than this. Hopefully it will get better with a higher level change. \n",
    "\n",
    "### Target length 100, vocab size 85k (all the 1-grams) (*wholepool_t100_emb85k(max)*)\n",
    "\n",
    "1. exp1: k1=2, k2=4, embedding_dim=100, out_channels=150. stacks with dropout1d@0.2\n",
    "    There's something here, the first model to dip slighly below 0.46 valid loss (0.459 sm). Performance otherwise similar to 50k embedding dim.\n",
    "\n",
    "\n",
    "### Target length 200, vocab size 85k (all the 1-grams) (*wholepool_t200_emb85k(max)*)\n",
    "1. exp1: k1=2, k2=4, embedding_dim=100, out_channels=150. stacks with dropout1d@0.2.\n",
    "    For now winning combo. Best valid loss 0.4319 and best valid acc just hit 0.8. Best results so far on test sets.\n",
    "\n",
    "### Target length 250, vocab size 85k (all the 1-grams) (*wholepool_t250_emb85k(max)*)\n",
    "1. exp1: k1=2, k2=4, embedding_dim=100, out_channels=150. stacks with dropout1d@0.2.\n",
    "    Keeps getting better consistently with increasing target length. Going to 300 didn't help much.\n",
    "\n",
    "2. exp2: **introduced one more conv layer with kernel_size=6**. so k1=2, k2=4, k3=6. out_channels (f_maps) = 100 (going to 150 with this wasnt good).\n",
    "    So far best results on both valid loss and acc during training (first time over 0.81 and validloss=0.4). \n",
    "\n",
    "\n",
    "3. exp3: same as exp2, with out_channels=120 (change from 100). best results so far. Stopping at epoch 3.\n",
    "\n",
    "4. exp4: same as exp3, with batch size=50. first model to dip below 0.4 valid loss. stopping at epoch 4. \n",
    "\n",
    "5. exp5: same as exp4, with batch size=40 and reduced number of rotten samples to be used as training data from 12k to 6k. best results.\n",
    "\n",
    "6. exp6: rotten 4k samples, batch size=32, out_channels=150. ok results, similiar to previous.\n",
    "\n",
    "7. exp7: changed embedding_dim=150, out_channels=200. similiar results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
