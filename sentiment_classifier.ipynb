{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The sentiment analysis model within this notebook uses publicly available datasets:\n",
    "\n",
    "1. Large Movie Review Dataset: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "2. Sentiment Polarity Dataset (v1 and v2): https://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "3. Rotten Tomatoes web scraped critic reviews: https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset/\n",
    "\n",
    "All data is contained in the \"data\" directory (not committed in this repo).\n",
    "The code below assumes the same directory structure as the original datasets under the root \"data\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataset_utils import *\n",
    "\n",
    "train_path = os.path.join(IMDB_DATA_PATH, \"train\")\n",
    "test_path  = os.path.join(IMDB_DATA_PATH, \"test\")\n",
    "\n",
    "\"\"\"\n",
    "For details on implementation of dataset loading and other utils check out dataset_utils.py\n",
    "\n",
    "CONVENTION: label 0 --> negative review\n",
    "            label 1 --> positive review\n",
    "\"\"\"\n",
    "\n",
    "#Main training set - Large Movie Review Dataset (IMDB)\n",
    "((imdb_train_texts, imdb_train_labels), (imdb_test_texts, imdb_test_labels)) = load_imdb_dataset(train_path=train_path, test_path=test_path)\n",
    "\n",
    "#Review Polarity Datasets\n",
    "(v1_texts, v1_labels), (v2_texts, v2_labels) = load_polarity(v1_path=POLARITY_v1_DATA_PATH, v2_path=POLARITY_v2_DATA_PATH)\n",
    "\n",
    "#Rotten tomatoes critic dataset\n",
    "rotten_train_texts, rotten_train_labels, short_rotten_test, random_rotten_test, pos_rotten_test, neg_rotten_test = load_rotten_split(ROTTEN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_len_dist(dataset_name, texts):\n",
    "    data=[len(text) for text in texts]\n",
    "    median = np.median(data)\n",
    "    plt.hist(data, 50)\n",
    "    plt.axvline(x=median, color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "    plt.text(median + 0.5, 50, f'Median: {median:.2f}', color='red', fontsize=10)\n",
    "    plt.title(f\"Length of sample over number of samples: {dataset_name}\")\n",
    "    plt.xlabel(\"Length of a sample\")\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_wordcount_dist(dataset_name, texts):\n",
    "    data=[len(text.split(\" \")) for text in texts]\n",
    "    median=np.median(data)\n",
    "    plt.hist(data, 50)\n",
    "    plt.axvline(x=median, color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "    plt.text(median + 0.5, 50, f'Median: {median:.2f}', color='red', fontsize=10)\n",
    "    plt.title(f\"Number of words: {dataset_name}\")\n",
    "    plt.xlabel(\"Number of words in a sample\")\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    plt.show()\n",
    "\n",
    "# for title, texts in [('IMDB_train', imdb_train_texts), ('IMDB_test', imdb_test_texts), ('polarity_v1', v1_texts), ('polarity_v2',v2_texts),\n",
    "#                      ('rotten_train', rotten_train_texts), ('rotten_test', rotten_test_texts)]:\n",
    "#     plot_len_dist(title, texts)\n",
    "#     plot_wordcount_dist(title, texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataset_info(texts, labels, name):\n",
    "    print(f\"Dataset: {name}.\")\n",
    "    print(f\"Total number of samples: {len(texts)}\")\n",
    "    print(f\"Positive reviews total: {sum(label==1 for label in labels)}\")\n",
    "    print(f\"Negative reviews total: {sum(label==0 for label in labels)}\")\n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "for texts, labels, name in [(imdb_train_texts, imdb_train_labels, 'imdb_train'), (imdb_test_texts, imdb_test_labels, 'imdb_test'),\n",
    "                            (v1_texts, v1_labels, 'polarity_v1'), (v2_texts, v2_labels, 'polarity_v2'),\n",
    "                            (rotten_train_texts, rotten_train_labels, 'rotten tomatoes train')]:\n",
    "    \n",
    "    display_dataset_info(texts, labels, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts_v1, test_texts_v1, train_labels_v1, test_labels_v1 = train_test_split(v1_texts, v1_labels, test_size=0.33, random_state=42)\n",
    "\n",
    "train_texts = imdb_train_texts + rotten_train_texts + train_texts_v1\n",
    "train_labels = imdb_train_labels + rotten_train_labels + train_labels_v1\n",
    "\n",
    "seed=10\n",
    "random.seed(seed)\n",
    "random.shuffle(train_texts)\n",
    "random.seed(seed)\n",
    "random.shuffle(train_labels)\n",
    "\n",
    "print(f\"Training set size: {len(train_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_test_v1 = [(test_labels_v1[index], text) for index, text in enumerate(test_texts_v1) if len(text.split(\" \")) < 25]\n",
    "rest_test_v1 = [(test_labels_v1[index], text) for index, text in enumerate(test_texts_v1) if len(text.split(\" \")) >= 25]\n",
    "short_test = short_test_v1 + short_rotten_test\n",
    "\n",
    "long_test_v2 = [(v2_labels[index], text) for index, text in enumerate(v2_texts) if len(text.split(\" \")) > 300]\n",
    "long_test_imdb = [(imdb_test_labels[index], text) for index, text in enumerate(imdb_test_texts) if len(text.split(\" \")) > 300]\n",
    "\n",
    "long_test = long_test_v2 + long_test_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "random.shuffle(short_test)\n",
    "random.seed(seed)\n",
    "random.shuffle(long_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "MAX_FEATURES = 5000\n",
    "\n",
    "transformer_pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(min_df=10, ngram_range=(1,2))),\n",
    "    ('select', SelectKBest(k=MAX_FEATURES, score_func=f_classif))\n",
    "])\n",
    "\n",
    "transformed_train = transformer_pipeline.fit_transform(train_texts, train_labels)\n",
    "\n",
    "X_train = torch.from_numpy(transformed_train.toarray()).to(dtype=torch.float32)\n",
    "y_train = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "vectorizer = transformer_pipeline.named_steps['vect']\n",
    "selector   = transformer_pipeline.named_steps['select']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "class SentimentCLF(nn.Module):\n",
    "    def __init__(self, n_units, dropout_p, n_hidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, n_units),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        from torch import nn\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "class SentimentCLF(nn.Module):\n",
    "    def __init__(self, n_units, dropout_p, n_hidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, n_units),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        for _ in range(n_hidden):\n",
    "            self.linear_stack.append(nn.Linear(n_units, n_units))\n",
    "            self.linear_stack.append(nn.LeakyReLU())\n",
    "            \n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        self.final_linear = nn.Linear(n_units, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear_stack(x)\n",
    "        dropped = self.dropout(out)\n",
    "        logits = self.final_linear(dropped)\n",
    "        \n",
    "        return logits\n",
    "        dropped = self.dropout(out)\n",
    "        logits = self.final_linear(dropped)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "For training, we find the ideal neural network for our needs by running experiments. Each experiment corresponds to a different model with different parameters.\n",
    "\n",
    "### Experiments directory structure\n",
    "\n",
    "Top directory: `models`. Within this directory there are multiple directories that correspond to different settings of the vectorizer (text encoding + feature selection). E.g. folder `min_df_10_ft_5000best` contains models for which the training data was encoded using tfidfvectorizer with parameters min_df=10 and then following that we select 5000best features (select k best).\n",
    "\n",
    "Within each of the directories corresponding to the vectorizer settings, there are multiple experiments: exp1, exp2, .. etc.\n",
    "Each experiment contains the model (pkl file), training history and results on the test sets at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import Checkpoint, LoadInitState, LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "model_dirs = os.listdir('models')\n",
    "\n",
    "#top directory regards vectorizer settings\n",
    "topdir = model_dirs[0]\n",
    "\n",
    "#experiments within chosen vectorizer settings directory\n",
    "experiments = sorted(os.listdir(f'models/{topdir}'))\n",
    "\n",
    "if len(experiments) == 0:\n",
    "    curr_number = 1\n",
    "else:\n",
    "    curr_number = int(experiments[-1][-1])\n",
    "    \n",
    "next_number = curr_number + 1\n",
    "\n",
    "dirname_curr=f'models/{topdir}/exp{curr_number}'\n",
    "f_pickle_curr=f'exp{curr_number}.pkl'\n",
    "\n",
    "dirname_next=f'models/{topdir}/exp{next_number}'\n",
    "f_pickle_next=f'exp{next_number}.pkl'\n",
    "\n",
    "#checkpoints for saving the model during training. cp_current corresponds to the last experiment.\n",
    "#When we switch the callback to cp_next, this starts a new experiment\n",
    "\n",
    "cp_current = Checkpoint(monitor='valid_loss_best', dirname=dirname_curr, f_pickle=f_pickle_curr)\n",
    "cp_next    = Checkpoint(monitor='valid_loss_best', dirname=dirname_next, f_pickle=f_pickle_next)\n",
    "load_state = LoadInitState(cp_current)\n",
    "lr_scheduler = ('lr_scheduler', LRScheduler(policy=ReduceLROnPlateau, mode='min', factor=0.1, patience=7))\n",
    "\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    module=SentimentCLF,\n",
    "    module__n_units=500,\n",
    "    module__dropout_p=0.3,\n",
    "    module__n_hidden=2,\n",
    "    lr = 0.00001,\n",
    "    criterion=nn.BCEWithLogitsLoss,   \n",
    "    device=device,\n",
    "    max_epochs=20,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer__weight_decay=0.0001,\n",
    "    batch_size=64,\n",
    "    iterator_train__shuffle=True,\n",
    "    callbacks = [cp_current, load_state],\n",
    "    #callbacks = [cp_next],\n",
    "    #callbacks=[cp_current]\n",
    ")\n",
    "\n",
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_params(checkpoint=cp_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_model_byexp_n(exp_number, vect_idx):\n",
    "\n",
    "    \"\"\"\n",
    "    A function loading the model from previously ran experiments.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    exp_number - experiment number within top directory.\n",
    "    vect_idx - index of the top directory regarding vectorizer settings.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    model - a skorch model.\n",
    "    exp_path - path to the chosen experiment directory. Used during testing for saving results.\n",
    "    \"\"\"\n",
    "    \n",
    "    topdir = model_dirs[vect_idx]\n",
    "\n",
    "    experiments = sorted(os.listdir(f'models/{topdir}'))\n",
    "    if len(experiments) == 0:\n",
    "        print(\"no experiments here\")\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            exp = experiments[exp_number-1]\n",
    "        except IndexError:\n",
    "            print(\"No such experiment\")\n",
    "            return None\n",
    "        \n",
    "        #path to experiment directory\n",
    "        exp_path = f'models/{topdir}/{exp}'\n",
    "        #path to model within exp_path        \n",
    "        pkl_path = os.path.join(exp_path, f'{exp}.pkl')\n",
    "\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            \n",
    "        return model, exp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "net.set_params(train_split=False, verbose=0)\n",
    "\n",
    "param_grid = {\n",
    "    'module__dropout_p': [0.4, 0.5],\n",
    "    'module__n_units': [500, 700, 1000, 2000],\n",
    "    'module__n_hidden': [1, 2, 3],\n",
    "    'lr': [0.0001]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(net, param_grid=param_grid, refit=False, cv=3, scoring='accuracy', verbose=3)\n",
    "#gs.fit(X_train, y_train)\n",
    "#print(gs.best_score_, gs.best_params_)\n",
    "#net.set_params(**gs.best_params_)\n",
    "#net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, exp_path = get_model_byexp_n(exp_number=curr_number, vect_idx=0)\n",
    "exp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "### Sets:\n",
    "- IMDB: Small, medium, large\n",
    "- Short reviews (in terms of number of tokens): Small, large\n",
    "- Long reviews\n",
    "- Rotten reviews, mixed (rotten are mostly short reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the imdb test reviews and their label into 3 datasets based on size:\n",
    "#small-1000, medium-5000, large-19000\n",
    "imdb_small_test_texts  = imdb_test_texts[:1000]\n",
    "imdb_small_test_labels = imdb_test_labels[:1000]\n",
    "\n",
    "imdb_medium_test_texts  = imdb_test_texts[1000:6000]\n",
    "imdb_medium_test_labels = imdb_test_labels[1000:6000]\n",
    "\n",
    "imdb_large_test_texts  = imdb_test_texts[6000:]\n",
    "imdb_large_test_labels = imdb_test_labels[6000:]\n",
    "\n",
    "#Splitting the short reviews dataset into 2 - sm and large \n",
    "short_test_sm = short_test[:3000]\n",
    "short_test_lg = short_test[3000:]\n",
    "\n",
    "short_test_sm_texts  = [text for _, text in short_test_sm]\n",
    "short_test_sm_labels = [label for label, _ in short_test_sm]\n",
    "\n",
    "short_test_lg_texts  = [text for _, text in short_test_lg]\n",
    "short_test_lg_labels = [label for label, _ in short_test_lg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_sets = []\n",
    "\n",
    "for (texts, labels, name) in [(imdb_small_test_texts, imdb_small_test_labels, 'imdb_sm'), (imdb_medium_test_texts, imdb_medium_test_labels, 'imdb_md'),\n",
    "                            (imdb_large_test_texts, imdb_large_test_labels, 'imdb_lg'), (short_test_sm_texts, short_test_sm_labels, 'short_sm'),\n",
    "                            (short_test_lg_texts, short_test_lg_labels, 'short_lg'),\n",
    "                            ([text for _, text in random_rotten_test], [label for label, _ in random_rotten_test], 'rotten_mix'),\n",
    "                            ([text for _, text in long_test], [label for label, _ in long_test], 'long (v2+imdb)'),\n",
    "                            ([text for _, text in rest_test_v1], [label for label, _ in rest_test_v1], 'rest v1(more than 25 tokens)'),\n",
    "                            (v2_texts, v2_labels, 'entire v2'),\n",
    "                            ([text for _, text in pos_rotten_test], [label for label, _ in pos_rotten_test], 'rotten_pos_only'),\n",
    "                            ([text for _, text in neg_rotten_test], [label for label, _ in neg_rotten_test], 'rotten_neg_only')]:\n",
    "    \n",
    "    transformed_text = transformer_pipeline.transform(texts).toarray()\n",
    "    X = torch.from_numpy(transformed_text).to(dtype=torch.float32)\n",
    "    y = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    encoded_test_sets.append((X, y, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_score\n",
    "\n",
    "\"\"\"\n",
    "The main single-number metric we're checking is accuracy. We show the confusion matrix on each test set as well as the precision score.\n",
    "Precision metric answers the question \"What proportion of positive predictions were classified correctly?\". The reason we check is that the model tends to\n",
    "mislabel reviews that were actually negative.\n",
    "\"\"\"\n",
    "\n",
    "def test_loop(X, y, set_name, model):\n",
    "    print(f\"DATASET: {set_name}\")\n",
    "    \n",
    "    pred = model.predict(X)\n",
    "    \n",
    "    acc = accuracy_score(y, pred)\n",
    "    precision = precision_score(y, pred)\n",
    "    cm = confusion_matrix(y, pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    \n",
    "    print(f\"Accuracy score: {acc}\")\n",
    "    print(f\"Precision score: {precision}\")\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    \n",
    "    print('-----------------------------------------------------')\n",
    "    \n",
    "    return (acc, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "\n",
    "for (X_test, y_test, name) in encoded_test_sets:\n",
    "    acc, precision = test_loop(X_test, y_test, set_name=name, model=model)\n",
    "    acc_key = f\"{name}_acc\"\n",
    "    prec_key = f\"{name}_prec\"\n",
    "    \n",
    "    results_dict[acc_key] = acc\n",
    "    results_dict[prec_key] = precision   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_results(exp_path, results_dict):\n",
    "    \n",
    "    res_path = os.path.join(exp_path, 'results.json')\n",
    "    json_data = json.dumps(results_dict, indent=1)\n",
    "    \n",
    "    if os.path.exists(res_path):\n",
    "        print(\"Results file already exists for this experiment. Aborting\")\n",
    "        return None\n",
    "    \n",
    "    with open(res_path, 'w') as f:\n",
    "        f.write(json_data)\n",
    "        \n",
    "write_results(exp_path=exp_path, results_dict=results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial conclusion: model tends to mislabel mostly negative reviews, thinking they're positive. It could be due to statements like:\n",
    "*Despite the good cinematography and competent X actor's performance, the movie lacks a gripping narrative and a point to exist besides making money.*\n",
    "\n",
    "In a review like this, most adjectives are positive bc of the structure like \"Some things good, v important thing bad, therefore movie bad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
