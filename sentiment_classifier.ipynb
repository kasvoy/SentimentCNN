{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The sentiment analysis model within this notebook uses publicly available datasets:\n",
    "\n",
    "1. Large Movie Review Dataset: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "2. Sentiment Polarity Dataset (v1 and v2): https://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "3. Rotten Tomatoes web scraped critic reviews: https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset/\n",
    "\n",
    "All data is contained in the \"data\" directory (not committed in this repo).\n",
    "The code below assumes the same directory structure as the original datasets under the root \"data\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataset_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(IMDB_DATA_PATH, \"train\")\n",
    "test_path  = os.path.join(IMDB_DATA_PATH, \"test\")\n",
    "\n",
    "#Main training set - Large Movie Review Dataset (IMDB)\n",
    "((imdb_train_texts, imdb_train_labels), (imdb_test_texts, imdb_test_labels)) = load_imdb_dataset(train_path=train_path, test_path=test_path)\n",
    "\n",
    "#Review Polarity Datasets - used as additional test data\n",
    "(v1_texts, v1_labels), (v2_texts, v2_labels) = load_polarity(v1_path=POLARITY_v1_DATA_PATH, v2_path=POLARITY_v2_DATA_PATH)\n",
    "\n",
    "#Rotten tomatoes critic dataset\n",
    "((rotten_train_texts, rotten_train_labels), (rotten_test_texts, rotten_test_labels)) = load_rotten(ROTTEN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_len_dist(dataset_name, texts):\n",
    "    plt.hist([len(text) for text in texts], 50)\n",
    "    plt.title(f\"Length of sample over number of samples: {dataset_name}\")\n",
    "    plt.xlabel(\"Length of a sample\")\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    plt.show()\n",
    "\n",
    "for title, texts in [('IMDB_train', imdb_train_texts), ('IMDB_test', imdb_test_texts), ('polarity_v1', v1_texts), ('polarity_v2',v2_texts),\n",
    "                     ('rotten_train', rotten_train_texts), ('rotten_test', rotten_test_texts)]:\n",
    "    plot_len_dist(title, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for texts, labels, name in [(imdb_train_texts, imdb_train_labels, 'imdb_train'), (imdb_test_texts, imdb_test_labels, 'imdb_test'),\n",
    "                            (v1_texts, v1_labels, 'polarity_v1'), (v2_texts, v2_labels, 'polarity_v2'),\n",
    "                            (rotten_train_texts, rotten_train_labels, 'rotten tomatoes train'), \n",
    "                            (rotten_test_texts, rotten_test_labels, 'rotten tomatoes test')]:\n",
    "    \n",
    "    display_dataset_info(texts, labels, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import chain\n",
    "\n",
    "# train_texts_rot = list(chain(imdb_train_texts, rotten_train_texts))\n",
    "# train_labels_rot = list(chain(imdb_train_labels, rotten_train_labels))\n",
    "\n",
    "# test_texts_rot = list(chain(imdb_test_texts, rotten_test_texts))\n",
    "# test_labels_rot = list(chain(imdb_test_labels, rotten_test_labels))\n",
    "\n",
    "\n",
    "# random.seed(1)\n",
    "# random.shuffle(train_texts_rot)\n",
    "# random.shuffle(train_labels_rot)\n",
    "\n",
    "# random.seed(1)\n",
    "# random.shuffle(test_texts_rot)\n",
    "# random.shuffle(test_labels_rot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "MAX_FEATURES = 5000\n",
    "\n",
    "transformer_pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(min_df=8, ngram_range=(1,2))),\n",
    "    ('select', SelectKBest(k=MAX_FEATURES))\n",
    "])\n",
    "\n",
    "transformed_train = transformer_pipeline.fit_transform(imdb_train_texts, imdb_train_labels)\n",
    "transfomed_test  = transformer_pipeline.transform(imdb_test_texts)\n",
    "\n",
    "X_train = torch.from_numpy(transformed_train.toarray()).to(dtype=torch.float32)\n",
    "X_test  = torch.from_numpy(transfomed_test.toarray()).to(dtype=torch.float32)\n",
    "\n",
    "X_test_v1 = torch.from_numpy(transformer_pipeline.transform(v1_texts).toarray()).to(dtype=torch.float32)\n",
    "X_test_v2 = torch.from_numpy(transformer_pipeline.transform(v2_texts).toarray()).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(imdb_train_labels, dtype=torch.float32).unsqueeze(1)\n",
    "y_test  = torch.tensor(imdb_test_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "y_test_v1 = torch.tensor(v1_labels, dtype=torch.float32)\n",
    "y_test_v2 = torch.tensor(v2_labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_pipeline.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = transformer_pipeline.named_steps['vect']\n",
    "selector   = transformer_pipeline.named_steps['select']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "class SentimentCLF(nn.Module):\n",
    "    def __init__(self, n_hidden, n_units, dropout_p):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_stack = nn.Sequential(nn.Linear(input_dim, n_units), nn.ReLU())\n",
    "        \n",
    "        for _ in range(n_hidden):\n",
    "            self.linear_stack.append(nn.Linear(n_units, n_units))\n",
    "            self.linear_stack.append(nn.ReLU())\n",
    "        \n",
    "        self.linear_stack.append(nn.Linear(n_units, 1))\n",
    "                    \n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dropped = self.dropout(x)\n",
    "        logits = self.linear_stack(dropped)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    module=SentimentCLF,\n",
    "    module__n_units=500,\n",
    "    module__dropout_p=0.6,\n",
    "    module__n_hidden=2,\n",
    "    lr = 0.0001,\n",
    "    criterion=nn.BCEWithLogitsLoss,   \n",
    "    device='cuda',\n",
    "    max_epochs=15,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# loading\n",
    "with open('net1.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module_.linear_stack[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "net.set_params(train_split=False, verbose=0)\n",
    "\n",
    "param_grid = {\n",
    "    'module__dropout_p': [0.6],\n",
    "    'module__n_units': [100, 500, 1000],\n",
    "    'module__n_hidden': [1, 2, 3],\n",
    "    'lr': [0.0001]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(net, param_grid=param_grid, refit=False, cv=3, scoring='accuracy', verbose=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.set_params(**gs.best_params_)\n",
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#with min_df = 5, ngram_range = (1,2) \n",
    "with open('net2.pkl', 'wb') as f:\n",
    "    pickle.dump(net, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for X, y in [(X_test, y_test), (X_test_v1, y_test_v1), (X_test_v2, y_test_v2)]:\n",
    "    pred = model.predict(X)\n",
    "    print(accuracy_score(pred, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "1. Mix in more data to the training set. Maybe web scraping?\n",
    "2. Grid search through more vectorizer options and inspect how it affects performance.\n",
    "3. Package with argparse or sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
