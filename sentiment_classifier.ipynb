{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The sentiment analysis model within this notebook uses publicly available datasets:\n",
    "\n",
    "1. Large Movie Review Dataset: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "2. Sentiment Polarity Dataset (v1 and v2): https://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n",
    "All data is contained in the \"data\" directory (not committed in this repo).\n",
    "The code below assumes the same directory structure as the original datasets under the root \"data\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataset_utils import *\n",
    "\n",
    "train_path = os.path.join(IMDB_DATA_PATH, \"train\")\n",
    "test_path  = os.path.join(IMDB_DATA_PATH, \"test\")\n",
    "\n",
    "#Main training set - Large Movie Review Dataset (IMDB)\n",
    "((train_texts, train_labels), (test_texts, test_labels)) = load_imdb_dataset(train_path=train_path, test_path=test_path)\n",
    "\n",
    "#Review Polarity Datasets - used as additional test data\n",
    "(v1_texts, v1_labels), (v2_texts, v2_labels) = load_polarity(v1_path=POLARITY_v1_DATA_PATH, v2_path=POLARITY_v2_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_len_dist(dataset_name, texts):\n",
    "    plt.hist([len(text) for text in texts], 50)\n",
    "    plt.title(f\"Length of sample over number of samples: {dataset_name}\")\n",
    "    plt.xlabel(\"Length of a sample\")\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    plt.show()\n",
    "\n",
    "for title, texts in [('IMDB_train', train_texts), ('IMDB_test', test_texts), ('polarity_v1', v1_texts), ('polarity_v2',v2_texts)]:\n",
    "    plot_len_dist(title, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "MAX_FEATURES = 5000\n",
    "\n",
    "transformer_pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(min_df=8, ngram_range=(1,2))),\n",
    "    ('select', SelectKBest(k=MAX_FEATURES))\n",
    "])\n",
    "\n",
    "transformed_train = transformer_pipeline.fit_transform(train_texts, train_labels)\n",
    "transfomed_test  = transformer_pipeline.transform(test_texts)\n",
    "\n",
    "X_train = torch.from_numpy(transformed_train.toarray()).to(dtype=torch.float32)\n",
    "X_test  = torch.from_numpy(transfomed_test.toarray()).to(dtype=torch.float32)\n",
    "\n",
    "X_test_v1 = torch.from_numpy(transformer_pipeline.transform(v1_texts).toarray()).to(dtype=torch.float32)\n",
    "X_test_v2 = torch.from_numpy(transformer_pipeline.transform(v2_texts).toarray()).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)\n",
    "y_test  = torch.tensor(test_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "y_test_v1 = torch.tensor(v1_labels, dtype=torch.float32)\n",
    "y_test_v2 = torch.tensor(v2_labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_pipeline.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = transformer_pipeline.named_steps['vect']\n",
    "selector   = transformer_pipeline.named_steps['select']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "class SentimentCLF(nn.Module):\n",
    "    def __init__(self, n_hidden, n_units, dropout_p):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_stack = nn.Sequential(nn.Linear(input_dim, n_units), nn.ReLU())\n",
    "        \n",
    "        for _ in range(n_hidden):\n",
    "            self.linear_stack.append(nn.Linear(n_units, n_units))\n",
    "            self.linear_stack.append(nn.ReLU())\n",
    "        \n",
    "        self.linear_stack.append(nn.Linear(n_units, 1))\n",
    "                    \n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dropped = self.dropout(x)\n",
    "        logits = self.linear_stack(dropped)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    module=SentimentCLF,\n",
    "    module__n_units=500,\n",
    "    module__dropout_p=0.6,\n",
    "    module__n_hidden=2,\n",
    "    lr = 0.0001,\n",
    "    criterion=nn.BCEWithLogitsLoss,   \n",
    "    device='cuda',\n",
    "    max_epochs=15,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "net.set_params(train_split=False, verbose=0)\n",
    "\n",
    "param_grid = {\n",
    "    'module__dropout_p': [0.6],\n",
    "    'module__n_units': [100, 500, 1000],\n",
    "    'module__n_hidden': [1, 2, 3],\n",
    "    'lr': [0.0001]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(net, param_grid=param_grid, refit=False, cv=3, scoring='accuracy', verbose=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.set_params(**gs.best_params_)\n",
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#with min_df = 5, ngram_range = (1,2) \n",
    "with open('net2.pkl', 'wb') as f:\n",
    "    pickle.dump(net, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for X, y in [(X_test, y_test), (X_test_v1, y_test_v1), (X_test_v2, y_test_v2)]:\n",
    "    pred = net.predict(X)\n",
    "    print(accuracy_score(pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
