{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The sentiment analysis model within this notebook uses publicly available datasets:\n",
    "\n",
    "1. Large Movie Review Dataset: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "2. Sentiment Polarity Dataset (v1 and v2): https://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "3. Rotten Tomatoes web scraped critic reviews: https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset/\n",
    "\n",
    "All data is contained in the \"data\" directory (not committed in this repo).\n",
    "The code below assumes the same directory structure as the original datasets under the root \"data\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataset_utils import *\n",
    "\n",
    "train_path = os.path.join(IMDB_DATA_PATH, \"train\")\n",
    "test_path  = os.path.join(IMDB_DATA_PATH, \"test\")\n",
    "\n",
    "#For details on implementation of dataset loading and other utils check out dataset_utils.py\n",
    "\n",
    "#Main training set - Large Movie Review Dataset (IMDB)\n",
    "((imdb_train_texts, imdb_train_labels), (imdb_test_texts, imdb_test_labels)) = load_imdb_dataset(train_path=train_path, test_path=test_path)\n",
    "\n",
    "#Review Polarity Datasets\n",
    "(v1_texts, v1_labels), (v2_texts, v2_labels) = load_polarity(v1_path=POLARITY_v1_DATA_PATH, v2_path=POLARITY_v2_DATA_PATH)\n",
    "\n",
    "#Rotten tomatoes critic dataset\n",
    "rotten_train_texts, rotten_train_labels, short_rotten_test, random_rotten_test = load_rotten_split(ROTTEN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_len_dist(dataset_name, texts):\n",
    "    data=[len(text) for text in texts]\n",
    "    median = np.median(data)\n",
    "    plt.hist(data, 50)\n",
    "    plt.axvline(x=median, color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "    plt.text(median + 0.5, 50, f'Median: {median:.2f}', color='red', fontsize=10)\n",
    "    plt.title(f\"Length of sample over number of samples: {dataset_name}\")\n",
    "    plt.xlabel(\"Length of a sample\")\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_wordcount_dist(dataset_name, texts):\n",
    "    data=[len(text.split(\" \")) for text in texts]\n",
    "    median=np.median(data)\n",
    "    plt.hist(data, 50)\n",
    "    plt.axvline(x=median, color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
    "    plt.text(median + 0.5, 50, f'Median: {median:.2f}', color='red', fontsize=10)\n",
    "    plt.title(f\"Number of words: {dataset_name}\")\n",
    "    plt.xlabel(\"Number of words in a sample\")\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    plt.show()\n",
    "\n",
    "# for title, texts in [('IMDB_train', imdb_train_texts), ('IMDB_test', imdb_test_texts), ('polarity_v1', v1_texts), ('polarity_v2',v2_texts),\n",
    "#                      ('rotten_train', rotten_train_texts), ('rotten_test', rotten_test_texts)]:\n",
    "#     plot_len_dist(title, texts)\n",
    "#     plot_wordcount_dist(title, texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: imdb_train.\n",
      "Total number of samples: 25000\n",
      "Positive reviews total: 12500\n",
      "Negative reviews total: 12500\n",
      "Dataset: imdb_test.\n",
      "Total number of samples: 25000\n",
      "Positive reviews total: 12500\n",
      "Negative reviews total: 12500\n",
      "Dataset: polarity_v1.\n",
      "Total number of samples: 10662\n",
      "Positive reviews total: 5331\n",
      "Negative reviews total: 5331\n",
      "Dataset: polarity_v2.\n",
      "Total number of samples: 2000\n",
      "Positive reviews total: 1000\n",
      "Negative reviews total: 1000\n",
      "Dataset: rotten tomatoes train.\n",
      "Total number of samples: 50000\n",
      "Positive reviews total: 25000\n",
      "Negative reviews total: 25000\n"
     ]
    }
   ],
   "source": [
    "for texts, labels, name in [(imdb_train_texts, imdb_train_labels, 'imdb_train'), (imdb_test_texts, imdb_test_labels, 'imdb_test'),\n",
    "                            (v1_texts, v1_labels, 'polarity_v1'), (v2_texts, v2_labels, 'polarity_v2'),\n",
    "                            (rotten_train_texts, rotten_train_labels, 'rotten tomatoes train')]:\n",
    "    \n",
    "    display_dataset_info(texts, labels, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts_v1, test_texts_v1, train_labels_v1, test_labels_v1 = train_test_split(v1_texts, v1_labels, test_size=0.33, random_state=42)\n",
    "\n",
    "train_texts = imdb_train_texts + rotten_train_texts + train_texts_v1\n",
    "train_labels = imdb_train_labels + rotten_train_labels + train_labels_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_test_v1 = [(test_labels_v1[index], text) for index, text in enumerate(test_texts_v1) if len(text.split(\" \")) < 25]\n",
    "short_test = short_test_v1 + short_rotten_test\n",
    "\n",
    "long_test_v2 = [(v2_labels[index], text) for index, text in enumerate(v2_texts) if len(text.split(\" \")) > 300]\n",
    "long_test_imdb = [(imdb_test_labels[index], text) for index, text in enumerate(imdb_test_texts) if len(text.split(\" \")) > 300]\n",
    "\n",
    "long_test = long_test_v2 + long_test_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "MAX_FEATURES = 5000\n",
    "\n",
    "transformer_pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(min_df=8, ngram_range=(1,2))),\n",
    "    ('select', SelectKBest(k=MAX_FEATURES))\n",
    "])\n",
    "\n",
    "transformed_train = transformer_pipeline.fit_transform(train_texts, train_labels)\n",
    "\n",
    "X_train = torch.from_numpy(transformed_train.toarray()).to(dtype=torch.float32)\n",
    "y_train = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "vectorizer = transformer_pipeline.named_steps['vect']\n",
    "selector   = transformer_pipeline.named_steps['select']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "class SentimentCLF(nn.Module):\n",
    "    def __init__(self, n_hidden, n_units, dropout_p):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_stack = nn.Sequential(nn.Linear(input_dim, n_units), nn.ReLU())\n",
    "        \n",
    "        for _ in range(n_hidden):\n",
    "            self.linear_stack.append(nn.Linear(n_units, n_units))\n",
    "            self.linear_stack.append(nn.ReLU())\n",
    "        \n",
    "        self.linear_stack.append(nn.Linear(n_units, 1))\n",
    "                    \n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dropped = self.dropout(x)\n",
    "        logits = self.linear_stack(dropped)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    module=SentimentCLF,\n",
    "    module__n_units=500,\n",
    "    module__dropout_p=0.6,\n",
    "    module__n_hidden=2,\n",
    "    lr = 0.0001,\n",
    "    criterion=nn.BCEWithLogitsLoss,   \n",
    "    device='cuda',\n",
    "    max_epochs=15,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    batch_size=64,\n",
    "    iterator_train__shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "net.set_params(train_split=False, verbose=0)\n",
    "\n",
    "param_grid = {\n",
    "    'module__dropout_p': [0.6],\n",
    "    'module__n_units': [100, 500, 1000],\n",
    "    'module__n_hidden': [1, 2, 3],\n",
    "    'lr': [0.0001]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(net, param_grid=param_grid, refit=False, cv=3, scoring='accuracy', verbose=3)\n",
    "#gs.fit(X_train, y_train)\n",
    "#print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.set_params(**gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
