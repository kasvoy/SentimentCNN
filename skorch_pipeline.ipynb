{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The sentiment analysis model within this notebook uses publicly available datasets:\n",
    "\n",
    "1. Large Movie Review Dataset: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "2. Sentiment Polarity Dataset (v1 and v2): https://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n",
    "All data is contained in the \"data\" directory (not committed in this repo).\n",
    "The code below assumes the same directory structure as the original datasets under the root \"data\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataset_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(IMDB_DATA_PATH, \"train\")\n",
    "test_path  = os.path.join(IMDB_DATA_PATH, \"test\")\n",
    "\n",
    "#Main training set - Large Movie Review Dataset (IMDB)\n",
    "((train_texts, train_labels), (test_texts, test_labels)) = load_dataset(train_path=train_path, test_path=test_path)\n",
    "\n",
    "#Review Polarity Datasets - used as additional test data\n",
    "(v1_texts, v1_labels), (v2_texts, v2_labels) = load_polarity(v1_path=POLARITY_v1_DATA_PATH, v2_path=POLARITY_v2_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import torch\n",
    "\n",
    "\n",
    "MAX_FEATURES = 5000\n",
    "\n",
    "transformer_pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(min_df=5, ngram_range=(1,2))),\n",
    "    ('select', SelectKBest(k=MAX_FEATURES))\n",
    "])\n",
    "\n",
    "X_train = transformer_pipeline.fit_transform(train_texts, train_labels).toarray()\n",
    "X_test  = transformer_pipeline.transform(test_texts).toarray()\n",
    "\n",
    "X_train = torch.from_numpy(X_train).to(dtype=torch.float32)\n",
    "X_test  = torch.from_numpy(X_test).to(dtype=torch.float32)\n",
    "\n",
    "X_test_v1 = torch.from_numpy(transformer_pipeline.transform(v1_texts).toarray()).to(dtype=torch.float32)\n",
    "X_test_v2 = torch.from_numpy(transformer_pipeline.transform(v2_texts).toarray()).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)\n",
    "y_test  = torch.tensor(test_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "y_test_v1 = torch.tensor(v1_labels, dtype=torch.float32)\n",
    "y_test_v2 = torch.tensor(v2_labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "class SentimentCLF(nn.Module):\n",
    "    def __init__(self, n_units, dropout_p):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, n_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_units, n_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_units, 1)\n",
    "        )\n",
    "            \n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        dropped = self.dropout(x)\n",
    "        logits = self.linear_stack(dropped)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    module=SentimentCLF,\n",
    "    module__n_units=100,\n",
    "    module__dropout_p=0.5,\n",
    "    lr = 0.001,\n",
    "    criterion=nn.BCEWithLogitsLoss,   \n",
    "    device='cuda',\n",
    "    max_epochs=15,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "net.set_params(train_split=False, verbose=0)\n",
    "param_grid = {\n",
    "    'net__lr': [0.001, 0.01],\n",
    "    'net__max_epochs': [10, 15, 20],\n",
    "    'net__module__n_units': [100, 500, 1000],\n",
    "}\n",
    "\n",
    "param_grid_1 = {\n",
    "    'module__dropout_p': [0.5, 0.6],\n",
    "    #'module__n_units': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(net, param_grid=param_grid_1, refit=False, cv=3, scoring='accuracy', verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "cv_res = copy(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_params_1 = {'module__dropout_p': 0.5, 'module__n_units': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.set_params(module__dropout_p=0.7)\n",
    "net.set_params(module__n_units=100)\n",
    "\n",
    "#net.initialize()\n",
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for X, y in [(X_test, y_test), (X_test_v1, y_test_v1), (X_test_v2, y_test_v2)]:\n",
    "    pred = net.predict(X)\n",
    "    print(accuracy_score(pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
